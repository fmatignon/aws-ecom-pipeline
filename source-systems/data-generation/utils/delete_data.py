"""
Remove all RDS tables, payments/shipments Parquet partitions, and the data-generation logs.

Assumes the environment is configured with AWS credentials and either:
  - RDS_SECRET_ARN pointing to credentials in Secrets Manager, or
  - explicit RDS_ENDPOINT/RDS_USERNAME/RDS_PASSWORD for local testing.
"""

from utils.state import get_bucket_name, get_log_prefix, get_s3_client
from datetime import datetime, UTC
from dotenv import load_dotenv
from pathlib import Path
from typing import Dict
import psycopg2
import boto3
import json
import sys
import os

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


load_dotenv(Path(__file__).parent.parent.parent.parent / ".env")


def log(msg: str) -> None:
    """
    Print a UTC timestamped informational message to stdout.

    Args:
        msg (str): Message describing the current section or status.
    """
    print(f"[{datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')}] {msg}")


def get_db_credentials() -> Dict[str, str]:
    """
    Retrieve RDS credentials from Secrets Manager using the configured ARN.
    Same method as data loading and ingestion pipeline.

    Returns:
        Dict[str, str]: Connection parameters for psycopg2.
    """
    secret_arn = os.environ.get("RDS_SECRET_ARN")

    if not secret_arn:
        raise RuntimeError("RDS_SECRET_ARN must be configured in the environment")

    region = os.environ.get("AWS_DEFAULT_REGION", "us-east-1")
    log(f"Starting: Retrieve RDS credentials via Secrets Manager ARN {secret_arn}")

    client = boto3.client("secretsmanager", region_name=region)
    secret_value = client.get_secret_value(SecretId=secret_arn)
    secret = json.loads(secret_value["SecretString"])

    # Required keys from the secret
    required_keys = ["host", "port", "username", "password"]
    missing_keys = [key for key in required_keys if key not in secret]
    if missing_keys:
        raise ValueError(f"RDS secret missing required keys: {', '.join(missing_keys)}")

    # Use database name from secret or fallback to default
    database_name = secret.get("dbname") or secret.get("database") or "ecommerce"

    log("Completed: Retrieved RDS credentials")
    return {
        "host": secret["host"],
        "port": int(secret["port"]),
        "database": database_name,
        "user": secret["username"],
        "password": secret["password"],
    }


def drop_tables() -> None:
    """
    Remove all ecommerce tables from the configured RDS database.
    """
    log("Starting: Drop RDS tables (order_items, orders, products, customers)")
    creds = get_db_credentials()
    conn = psycopg2.connect(**creds, connect_timeout=10)
    try:
        with conn.cursor() as cur:
            for table in ("order_items", "orders", "products", "customers"):
                cur.execute(f"DROP TABLE IF EXISTS {table} CASCADE")
                log(f"Completed: Dropped table {table}")
        conn.commit()
    finally:
        conn.close()
    log("Completed: Drop RDS tables")


def delete_s3_prefix(prefix: str) -> None:
    """
    Delete all objects stored under the provided S3 prefix.

    Args:
        prefix (str): S3 key prefix to delete.
    """
    bucket = os.environ["S3_BUCKET_NAME"]
    log(f"Starting: Delete objects under S3 prefix {prefix}")
    client = boto3.client(
        "s3", region_name=os.environ.get("AWS_DEFAULT_REGION", "us-east-1")
    )
    paginator = client.get_paginator("list_objects_v2")
    objects: list[Dict[str, str]] = []
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            objects.append({"Key": obj["Key"]})
    if objects:
        for i in range(0, len(objects), 1000):
            client.delete_objects(
                Bucket=bucket,
                Delete={"Objects": objects[i : i + 1000]},
            )
        log(f"Completed: Deleted {len(objects):,} objects under {prefix}")
    else:
        log(f"Completed: No objects found under {prefix}")


def delete_operation_logs() -> None:
    """
    Remove all operation log files generated by `main.py` from S3.
    """
    prefix = get_log_prefix()
    client = get_s3_client()
    bucket = get_bucket_name()

    log(f"Starting: Delete operation logs under {prefix}")
    paginator = client.get_paginator("list_objects_v2")
    objects: list[Dict[str, str]] = []
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            objects.append({"Key": obj["Key"]})
    if objects:
        for i in range(0, len(objects), 1000):
            client.delete_objects(
                Bucket=bucket,
                Delete={"Objects": objects[i : i + 1000]},
            )
        log(f"Completed: Deleted {len(objects):,} operation logs under {prefix}")
    else:
        log(f"Completed: No operation logs found under {prefix}")


def main() -> None:
    """
    Coordinate RDS table drops, S3 partition deletes, and log pruning.
    """
    log("Starting: Full cleanup run")
    try:
        drop_tables()
        delete_s3_prefix("source/payments/")
        delete_s3_prefix("source/shipments/")
        delete_operation_logs()
        log("Completed: Full cleanup run")
    except Exception as exc:
        log(f"Error in cleanup run: {exc}")
        raise


if __name__ == "__main__":
    main()
